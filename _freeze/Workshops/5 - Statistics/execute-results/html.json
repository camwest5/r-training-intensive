{
  "hash": "0be6ac8fb54c2573cb6c6c41b76d20f6",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Statistics\neditor: source\ndate: today\n---\n\nThis session is aimed as an overview of how to perform some statistical modelling with R. **It is an R workshop, not a statistics workshop** - if you'd like to better understand the statistical models, or need help deciding what's best for you, please consult a statistics resource or contact a statistician.\n\nIn this session, we'll cover\n\n- Descriptive statistics\n  - Measures of central tendancy\n  - Measures of variability\n  - Measures of correlation\n  \n- Inferential statistics\n  - Linear regressions\n  - Calculating confidence intervals\n  - T-tests\n  - $\\chi^2$ test\n  - ANOVAs\n\nWe'll be working from our [\"Players2024\" dataset](../data_sources/Players2024.csv). After downloading it and putting it in your data folder, to bring it in and clean it up,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(dplyr)\nplayers <- read.csv(\"data/Players2024.csv\")\nplayers <- players %>% filter(positions != \"Missing\", height_cm > 100)\n```\n:::\n\n\n\n::: {.cell}\n::: {.cell-output .cell-output-stderr}\n\n```\n\nAttaching package: 'dplyr'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n\n\n:::\n:::\n\n\n\n## Descriptive Statistics\n\nWe'll start with sample size. To calculate the number of non-empty observations in a column, say the numeric variable `players$height_cm`, we use the `length()` function\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlength(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 5932\n```\n\n\n:::\n:::\n\n\nWe can compute measures of central tendancy similarly. The average value is given by\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmean(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 183.0413\n```\n\n\n:::\n:::\n\n\nand the median by\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmedian(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 183\n```\n\n\n:::\n:::\n\n\n### Measures of variance\n\nWe can also compute measures of variance. The minimum and maximum are as expected\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmin(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 160\n```\n\n\n:::\n\n```{.r .cell-code}\nmax(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 206\n```\n\n\n:::\n:::\n\n\nThe function `range()` yields both\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrange(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 160 206\n```\n\n\n:::\n:::\n\n\nSo the actual range, i.e. the difference, is\n\n\n::: {.cell}\n\n```{.r .cell-code}\ndiff(range(players$height_cm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 46\n```\n\n\n:::\n:::\n\n\nQuartiles are given by `quantile()` and the inter-quartile range (IQR)  by `IQR()`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nquantile(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n  0%  25%  50%  75% 100% \n 160  178  183  188  206 \n```\n\n\n:::\n\n```{.r .cell-code}\nIQR(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 10\n```\n\n\n:::\n:::\n\n\nA column's standard deviation and variance are given by\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsd(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 6.838736\n```\n\n\n:::\n\n```{.r .cell-code}\nvar(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 46.76832\n```\n\n\n:::\n:::\n\n\nAll together, you can see a nice statistical summary with\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(players$height_cm)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n    160     178     183     183     188     206 \n```\n\n\n:::\n:::\n\n\n### Measures of correlation\n\nIf you've got two numeric variables, you might want to examine covariance and correlation. These indicate how strongly the variables are linearly related. We'll need to use the `players$age` variable as well.\n\nThe covariance between \"height_cm\" and \"age\" is\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncov(players$height_cm, players$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.5126608\n```\n\n\n:::\n:::\n\n\nSimilarly, we can find the Pearson correlation coefficient between two columns. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(players$height_cm, players$age)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.01682598\n```\n\n\n:::\n:::\n\n\nYou can also specify \"kendall\" or \"spearman\" for their respective correlation coefficients\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncor(players$height_cm, players$age, method = \"kendall\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.005417946\n```\n\n\n:::\n\n```{.r .cell-code}\ncor(players$height_cm, players$age, method = \"spearman\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.007604345\n```\n\n\n:::\n:::\n\n\n### Reminder about groupbys\n\nBefore we move to inferential statistics, it's worth reiterating the power of groupbys discussed in the second workshop.\n\nTo group by a specific variable, like \"positions\", we use \n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayers %>% \n    group_by(positions)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5,932 × 7\n# Groups:   positions [4]\n   name                birth_date height_cm positions  nationality   age club   \n   <chr>               <chr>          <dbl> <chr>      <chr>       <int> <chr>  \n 1 James Milner        1986-01-04       175 Midfield   England        38 Bright…\n 2 Anastasios Tsokanis 1991-05-02       176 Midfield   Greece         33 Volou …\n 3 Jonas Hofmann       1992-07-14       176 Midfield   Germany        32 Bayer …\n 4 Pepe Reina          1982-08-31       188 Goalkeeper Spain          42 Calcio…\n 5 Lionel Carole       1991-04-12       180 Defender   France         33 Kayser…\n 6 Ludovic Butelle     1983-04-03       188 Goalkeeper France         41 Stade …\n 7 Daley Blind         1990-03-09       180 Defender   Netherlands    34 Girona…\n 8 Craig Gordon        1982-12-31       193 Goalkeeper Scotland       41 Heart …\n 9 Dimitrios Sotiriou  1987-09-13       185 Goalkeeper Greece         37 Omilos…\n10 Alessio Cragno      1994-06-28       184 Goalkeeper Italy          30 Associ…\n# ℹ 5,922 more rows\n```\n\n\n:::\n:::\n\n\nBy applying our statistics to the `group_by` object, we'll apply them to *every* variable for *each* position.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayers %>% \n    group_by(positions) %>% \n    summarise(mean_height = mean(height_cm))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 2\n  positions  mean_height\n  <chr>            <dbl>\n1 Attack            181.\n2 Defender          184.\n3 Goalkeeper        191.\n4 Midfield          180.\n```\n\n\n:::\n:::\n\n\n## Inferential Statistics\n\nWhile descriptive statistics describes the data definitively, inferential statistics aim to produce models for extrapolating conlusions.\n\n### Simple linear regressions\n\nLeast-squares regression for two sets of measurements can be performed with the function `lm`. Recall that linear regressions have the mathematical form\n\n$$ Y = β_1 X + β_0 $$\n\nand we use the regression tool to estimate the parameters $β_0\\,,β_1$. We can equivalently say that $Y \\sim X$, which is what R takes in\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlm(\"height_cm ~ age\", players)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = \"height_cm ~ age\", data = players)\n\nCoefficients:\n(Intercept)          age  \n  182.38260      0.02583  \n```\n\n\n:::\n:::\n\n\nIf we store this as a variable, we can then produce a summary of the results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel <- lm(\"height_cm ~ age\", players)\nsummary(model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = \"height_cm ~ age\", data = players)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-23.028  -5.028   0.075   4.978  23.075 \n\nCoefficients:\n             Estimate Std. Error t value Pr(>|t|)    \n(Intercept) 182.38260    0.51599 353.460   <2e-16 ***\nage           0.02583    0.01993   1.296    0.195    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6.838 on 5930 degrees of freedom\nMultiple R-squared:  0.0002831,\tAdjusted R-squared:  0.0001145 \nF-statistic: 1.679 on 1 and 5930 DF,  p-value: 0.1951\n```\n\n\n:::\n:::\n\n\nIf you want to get specific parameters out, we can index with `$`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(model)$r.squared\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.0002831136\n```\n\n\n:::\n:::\n\n\nThat's a pretty shocking fit.\n\n#### Plotting it\n\nNaturally, you'd want to plot this. We'll need to use techniques from the visualisation session. Let's import **ggplot2**\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n```\n:::\n\n\nStart by making a scatterplot of the data,\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(players, aes(x = height_cm, y = age)) +\n  geom_point()\n```\n\n::: {.cell-output-display}\n![](5---Statistics_files/figure-html/unnamed-chunk-21-1.png){width=672}\n:::\n:::\n\n\nThen, you'll need to plot the regression as a line. For reference,\n\n$$ y = \\text{slope}\\times x + \\text{intercept}$$\n\nSo\n\n::: {.cell}\n\n```{.r .cell-code}\nb0 <- model$coefficients[1]\nb1 <- model$coefficients[2]\n\nggplot(players, aes(x = age, y = height_cm)) + \n    geom_point() + \n    geom_abline(intercept = b0, slope = b1)\n```\n\n::: {.cell-output-display}\n![](5---Statistics_files/figure-html/unnamed-chunk-22-1.png){width=672}\n:::\n:::\n\n\n### $t$-tests\n\nWe can also perform $t$-tests. Typically, these are performed to examine the statistical signficance of a difference between two samples' means. Let's examine whether that earlier groupby result for is accurate for heights, specifically, **are goalkeepers taller than non-goalkeepers?**\n\nLet's start by creating a new column with the values\n \n| | | \n| --- | --- |\n| `FALSE` | Non-goalkeeper | \n| `TRUE` | Goalkeeper | \n \n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayers <- players %>% \n  mutate(gk = positions == \"Goalkeeper\")\n```\n:::\n\n\nThe $t$-test's goal is to check whether $\\text{height\\_cm}$ depends on $\\text{gk}$, so the formula is $\\text{height\\_cm}\\sim\\text{gk}$. This is given to the `t.test` function:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nt.test(height_cm ~ gk, data = players)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tWelch Two Sample t-test\n\ndata:  height_cm by gk\nt = -48.817, df = 1274.4, p-value < 2.2e-16\nalternative hypothesis: true difference in means between group FALSE and group TRUE is not equal to 0\n95 percent confidence interval:\n -9.036644 -8.338391\nsample estimates:\nmean in group FALSE  mean in group TRUE \n           181.9810            190.6685 \n```\n\n\n:::\n:::\n\n\nYielding a p-value of $p<2.2\\times10^{-16}$, indicating that the null-hypothesis (*heights are the same*) is extremely unlikely.\n\nTo visualise this result, it might be helpful to produce a histogram of the heights\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(players, \n       aes(x = height_cm, fill = gk)) + \n  geom_histogram(bins = 24)\n```\n\n::: {.cell-output-display}\n![](5---Statistics_files/figure-html/unnamed-chunk-25-1.png){width=672}\n:::\n:::\n\n\n### ANOVAs\n\nWhat about the means of the other three? We could use an ANOVA to examine them. We use the `aov()` function for this. \n\nLet's start by making a new dataset without goalkeepers\n\n\n::: {.cell}\n\n```{.r .cell-code}\nno_gk <- players %>% filter(gk == FALSE)\n```\n:::\n\n\nNext, we save the analysis of variance results\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_aov <- aov(height_cm ~ positions, data = no_gk)\n```\n:::\n\n\nAnd examine them with `summary()`\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(res_aov)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n              Df Sum Sq Mean Sq F value Pr(>F)    \npositions      2  15470    7735   199.7 <2e-16 ***\nResiduals   5205 201552      39                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n```\n\n\n:::\n:::\n\n\nEven without goalkeepers included, it looks like their positions are not all independent of height.\n\n### $\\chi^2$ tests\n\n$χ^2$ tests are useful for examining the relationship of categorical variables by comparing the frequencies of each. Often, you'd use this if you can make a contingency table.\n\nWe only have one useful categorical variable here, \"positions\" (the others have too many unique values), so we'll need to create another. Let's see if there's a relationship between players' positions and names with the letter \"a\".\n\nMake a binary column for players with the letter \"a\" in their names. To do this, we need to apply a string method to *all* the columns in the dataframe as follows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayers <- players %>%\n  mutate(a_in_name = grepl(\"a\", name))\n```\n:::\n\n\n> The `grepl` function perform pattern matching: it checks if the pattern `\"a\"` is inside the values in `name`.\n\nLet's cross tabulate positions with this new column\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntable(players$positions, players$a_in_name)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n            \n             FALSE TRUE\n  Attack       291 1280\n  Defender     355 1606\n  Goalkeeper   149  575\n  Midfield     312 1364\n```\n\n\n:::\n:::\n\n\nThe $χ^2$ test's job is to examine whether players' positions depend on the presence of \"a\" in their name. To evaluate it we need to send the contingency table in:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nchisq.test(table(players$positions, players$a_in_name))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\n\tPearson's Chi-squared test\n\ndata:  table(players$positions, players$a_in_name)\nX-squared = 2.1808, df = 3, p-value = 0.5357\n```\n\n\n:::\n:::\n\n\nAs expected, there is no signifcant relationship. A simple bar plot can help us here\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(players,\n       aes(x = positions, fill = a_in_name)) + \n  geom_bar()\n```\n\n::: {.cell-output-display}\n![](5---Statistics_files/figure-html/unnamed-chunk-32-1.png){width=672}\n:::\n:::\n\n\nIf we use the `position = \"fill\"` parameter to `geom_bar`, we'll see each as a proportion\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(players,\n       aes(x = positions, fill = a_in_name)) + \n  geom_bar(position = \"fill\")\n```\n\n::: {.cell-output-display}\n![](5---Statistics_files/figure-html/unnamed-chunk-33-1.png){width=672}\n:::\n:::\n\n\nIt looks as though the proportions are much the same.\n\n### Generalised linear models\n\nWe'll finish by looking at Generalised Linear Models. The distributions they include are\n\n* Binomial\n* Poisson\n* Gaussian (Normal)\n* Gamma\n* Inverse Gaussian\n* A few *quasi* options\n\nWe'll use the *binomial* option to create logistic regressions.\n\nLogistic regressions examine the distribution of binary data. For us, we can compare the heights of **goalkeepers vs non-goalkeepers** again. \n\nNow, we can model this column with height. We'll do the same as our $t$-test case, but this time we need to specify that `family = binomial` to ensure we'll get a logistic:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nres_logistic <- glm(gk ~ height_cm, family = binomial, data = players)\n```\n:::\n\n\nWe can take a look at the results with\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(res_logistic)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nglm(formula = gk ~ height_cm, family = binomial, data = players)\n\nCoefficients:\n             Estimate Std. Error z value Pr(>|z|)    \n(Intercept) -53.23360    1.92714  -27.62   <2e-16 ***\nheight_cm     0.27448    0.01019   26.94   <2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 4401.4  on 5931  degrees of freedom\nResidual deviance: 3167.0  on 5930  degrees of freedom\nAIC: 3171\n\nNumber of Fisher Scoring iterations: 6\n```\n\n\n:::\n:::\n\n\nAnd we can then visualise it with `ggplot2`. We need to make *another* variable, because we need to replace `TRUE` $\\rightarrow$ `1` and `FALSE` $\\rightarrow$ `0` for the plot.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplayers <- players %>% mutate(gk_numeric = as.numeric(gk))\n```\n:::\n\n\nNow we can plot the logistic regression. The fitted values (on the $y$-axis) are stored in `res_logistic$fitted.values`, but there are no provided $x$-values - these come from the `players` dataset. Use `geom_point()` for the data and `geom_line()` for the fit:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(players, aes(x = height_cm, y = gk_numeric)) + \n  geom_point() + \n  geom_line(aes(y = res_logistic$fitted.values))\n```\n\n::: {.cell-output-display}\n![](5---Statistics_files/figure-html/unnamed-chunk-37-1.png){width=672}\n:::\n:::\n",
    "supporting": [
      "5---Statistics_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}